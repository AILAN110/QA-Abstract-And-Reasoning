{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert进行分类任务\n",
    "### 目录介绍\n",
    "    home/dataset:存放数据目录\n",
    "    home/work:存放vocab.txt、Bert预训练模型结构、config文件\n",
    "    home/model:存放训练存储的模型参数\n",
    "    \n",
    "### 其他\n",
    "    主要工具：采用transformers来加载模型与配置文件\n",
    "    * 加载tokenier:\n",
    "        BertTokenizer.from_pretrained(file_path)  \n",
    "            #file_path='bert-base-uncased'     (线上)\n",
    "            #file_path='bert-base-uncased-xx.txt'   (现下自定义)\n",
    "            #file_path='xxx'    (xxx为目录，目录必保存vocab.txt)\n",
    "       * tokenizer.encode(item[1],add_special_tokens=False)\n",
    "               等价于 tokenizer.tokenize(sentence)分词 + tokenizer.convert_tokens__to_ids()\n",
    "    * 加载config:\n",
    "        BertConfig.from_pretrained(file_path)  \n",
    "            #file_path=''     (同上)\n",
    "    * 加载model:\n",
    "        model=BertForSequenceClassification(config=config)\n",
    "        model=BertForSequenceClassification.from_pretrained(file_path)  \n",
    "            #file_path=''     (同上)\n",
    "    框架：pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,BertConfig,AdamW,get_linear_schedule_with_warmup\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数\n",
    "EPOCHS=10  #训练轮数\n",
    "BATCH_SIZE=4 #批次\n",
    "MAX_LEN=300 #文本长度\n",
    "LR=1e-5  #学习率\n",
    "WARMUP_STEPS=150  #预热步数\n",
    "T_TOTAL=1834 #总步数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义dataset类\n",
    "class My_dataset(Dataset):\n",
    "    def __init__(self,data_list):\n",
    "        self.dataset=data_list\n",
    "    \n",
    "    def __getitem__(self,item_index):\n",
    "        #构建一条x,y\n",
    "        text=self.dataset[item_index][1]\n",
    "        label=self.dataset[item_index][2]\n",
    "        return text,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "#加载数据,生成data_list,有问题????\n",
    "def load_dataset(file_path,max_len):\n",
    "    data_list=[]\n",
    "    r=csv.reader(open(file_path,'r',encoding='utf-8'))\n",
    "    for item in r:\n",
    "        if r.line_num==1:\n",
    "            continue\n",
    "        data_list.append(item)\n",
    "    #加载tokenizer\n",
    "    tokenizer=BertTokenizer.from_pretrained(\"home/work\")\n",
    "    #padding与切割操作\n",
    "    for item in data_list:\n",
    "        item[1]=item[1].strip().replace(\" \",\"\")  #去空格\n",
    "        num=max_len-len(item[1])\n",
    "        if num<0:   #切割\n",
    "            item[1]=item[1][:max_len]\n",
    "        else:       #填充\n",
    "            for _ in range(num):\n",
    "                item[1]=item[1]+\"[PAD]\"\n",
    "        item[1]=tokenizer.encode(item[1],add_special_tokens=False)\n",
    "        num_temp=max_len-len(item[1])   #再次检查\n",
    "        if num_temp>0:\n",
    "            for _ in range(num_temp):\n",
    "                item[1].append(0)\n",
    "        item[1]=[101]+item[1][:max_len]+[102]\n",
    "        item[1]=str(item[1])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7337\n"
     ]
    }
   ],
   "source": [
    "#数据封装dataloader\n",
    "train_data=load_dataset(\"home/dataset/Train.csv\",max_len=MAX_LEN)\n",
    "test_data=load_dataset(\"home/dataset/Test.csv\",max_len=MAX_LEN)\n",
    "train_dataset=My_dataset(train_data)\n",
    "print(len(train_data))\n",
    "train_loader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at home/work were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at home/work and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model:可改装成gpu训练\n",
    "config=BertConfig.from_pretrained(\"home/work\")   #加载config\n",
    "config.num_labels=3    #类别数\n",
    "# model=BertForSequenceClassification(config)\n",
    "model=BertForSequenceClassification.from_pretrained(\"home/work\",config=config)   #加载模型\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer=AdamW(model.parameters(),lr=LR,correct_bias=False)\n",
    "scheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=WARMUP_STEPS,num_training_steps=T_TOTAL)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准确率计算\n",
    "def batch_accuracy(pre,label):\n",
    "    pre=pre.argmax(dim=1)\n",
    "    correct=torch.eq(pre,label).sum().float().item()\n",
    "    accuracy=correct/float(len(label))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练\n",
    "def train(model,train_loader,optimizer,scheduler):\n",
    "    model.train()\n",
    "    best_acc=0.0\n",
    "    if os.path.exists(\"model/bert_cla.ckpt\"):\n",
    "        model.load_state_dict(torch.load('model/bert_cla.ckpt'))\n",
    "    print(\"开始训练。。。\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        count=0\n",
    "        for text,label in train_loader:\n",
    "            text_list=list(map(json.loads,text))   #加载\n",
    "            label_list=list(map(json.loads,label))\n",
    "#             print(text_list)\n",
    "            text_tensor=torch.tensor(text_list)  #转tensor\n",
    "            label_tensor=torch.tensor(label_list)\n",
    "            outputs=model(text_tensor,labels=label_tensor)   #模型训练\n",
    "            loss, logits = outputs[:2]\n",
    "            optimizer.zero_grad()   #梯度清空\n",
    "            loss.backward()         #求解梯度\n",
    "            scheduler.step()        \n",
    "            optimizer.step()\n",
    "\n",
    "            acc=batch_accuracy(logits,label_tensor)\n",
    "            print('epoch:{} | acc:{} | loss:{}'.format(epoch, acc, loss))\n",
    "#             if count%100==0:\n",
    "#                 print('epoch:{} | acc:{} | loss:{}'.format(epoch, acc, loss))\n",
    "            count+=1\n",
    "            if acc>=best_acc:\n",
    "                best_acc=acc\n",
    "                torch.save(model.state_dict(),\"bert_cla.ckpt\")\n",
    "    print('训练完成...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练。。。\n",
      "epoch:0 | acc:0.75 | loss:0.9358950853347778\n",
      "epoch:0 | acc:0.5 | loss:0.8102744221687317\n",
      "epoch:0 | acc:0.75 | loss:0.6711229085922241\n",
      "epoch:0 | acc:0.75 | loss:0.8209071159362793\n",
      "epoch:0 | acc:0.5 | loss:1.00160551071167\n",
      "epoch:0 | acc:0.75 | loss:0.796048641204834\n",
      "epoch:0 | acc:0.5 | loss:0.8536520600318909\n",
      "epoch:0 | acc:0.75 | loss:0.8585497140884399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-457de94b1f7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-90-fe7b6f6d57ab>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#梯度清空\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m#求解梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,train_loader,optimizer,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
