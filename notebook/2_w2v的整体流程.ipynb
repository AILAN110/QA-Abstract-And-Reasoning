{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量\n",
    "## 任务介绍\n",
    "* 原始数据预处理,生成标准的train、test数据\n",
    "* 生成词典vocab.txt\n",
    "* word2vec模型训练\n",
    "\n",
    "## 目录介绍\n",
    "* datasets目录：存放train、test数据的csv格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from jieba import posseg\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#切词\n",
    "def segment(sentence,cut=jieba.lcut,cut_type=\"word\",pos=False):\n",
    "    \"\"\"\n",
    "    切词\n",
    "    :param sentence:\n",
    "    :param cut:切词方法\n",
    "    :param cut_type: 'word' use jieba.lcut; 'char' use list(sentence)\n",
    "    :param pos: 词性切词\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    if pos:\n",
    "        if cut_type==\"word\":   #中文\n",
    "            word_pos_seq=posseg.lcut(sentence)\n",
    "            word_seq=[w for w,_ in word_pos_seq]\n",
    "            pos_seq=[p for _,p in word_pos_seq]\n",
    "            return word_seq,pos_seq\n",
    "        else:                #英文\n",
    "            word_seq=list(sentence)\n",
    "            pos_seq=[posseg.lcut(w)[0].flag for w in word_seq]\n",
    "            return word_seq,pos_seq\n",
    "    else:\n",
    "        if cut_type==\"word\":\n",
    "            return cut(sentence)\n",
    "        else:\n",
    "            return list(sentenceS)\n",
    "\n",
    "REMOVE_WORDS = ['|', '[', ']', '语音', '图片', ' ']\n",
    "\n",
    "#去除停用词\n",
    "def remove_words(words_list):\n",
    "    return [word for word in words_list if word not in REMOVE_WORDS]\n",
    "\n",
    "#切词并去除停用词\n",
    "def preprocess_sentence(sentence):\n",
    "    sen_list=segment(sentence,cut_type=\"word\")\n",
    "    sen_list=remove_words(sen_list)\n",
    "    return \" \".join(sen_list)\n",
    "\n",
    "#数据预处理\n",
    "def parse_data(train_path,test_path):\n",
    "    def handle(path,train=True):\n",
    "        data_df=pd.read_csv(path,encoding=\"utf-8\")\n",
    "        if train:\n",
    "            data_df.dropna(subset=['Report'],how='any',inplace=True)  #label去除有NAN的\n",
    "        data_df.fillna('',inplace=True)  #nan用‘’替换\n",
    "        x=data_df.Question.str.cat(data_df.Dialogue) #对话拼接\n",
    "        x=x.apply(preprocess_sentence)   #分词\n",
    "        if train:\n",
    "            y=data_df.Report.apply(preprocess_sentence)  #label\n",
    "            print('train_x is ', len(x))\n",
    "            print('train_y is ', len(y))\n",
    "            x.to_csv(\"datasets/train_seg_x.txt\",index=None,header=False)\n",
    "            y.to_csv(\"datasets/train_seg_y.txt\",index=None,header=False)\n",
    "        else:\n",
    "            print('test_x is ', len(x))\n",
    "            x.to_csv(\"datasets/test_seg_x.txt\",index=None,header=False)\n",
    "    handle(train_path)\n",
    "    handle(test_path,train=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.706 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x is  82873\n",
      "train_y is  82873\n",
      "test_x is  20000\n"
     ]
    }
   ],
   "source": [
    "train_path=\"datasets/AutoMaster_TrainSet.csv\"\n",
    "test_path=\"datasets/AutoMaster_TestSet.csv\"\n",
    "parse_data(train_path,test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存词典\n",
    "def save_vocab(vocab,path):\n",
    "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for wv in vocab:\n",
    "            w,v=wv\n",
    "            f.write(\"%s\\t%s\\n\"%(w,v))\n",
    "            \n",
    "#读取数据\n",
    "def read_data(path_1, path_2, path_3):\n",
    "    with open(path_1, 'r', encoding='utf-8') as f1, \\\n",
    "            open(path_2, 'r', encoding='utf-8') as f2, \\\n",
    "            open(path_3, 'r', encoding='utf-8') as f3:\n",
    "        words = []\n",
    "        sentences=[]\n",
    "        # print(f1)\n",
    "        for line in f1:\n",
    "            words += line.split()\n",
    "            sentences.append(line.strip())\n",
    "        for line in f2:\n",
    "            words += line.split(' ')\n",
    "            sentences.append(line.strip())\n",
    "        for line in f3:\n",
    "            words += line.split(' ')\n",
    "            sentences.append(line.strip())\n",
    "        print(len(words))\n",
    "    return words,sentences\n",
    "\n",
    "#构建词典\n",
    "def build_vocab(items, sort=True, min_count=0, lower=False):\n",
    "    \"\"\"\n",
    "    构建词典列表\n",
    "    :param items: list  [item1, item2, ... ]\n",
    "    :param sort: 是否按频率排序，否则按items排序\n",
    "    :param min_count: 词典最小频次\n",
    "    :param lower: 是否小写\n",
    "    :return: list: word set\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if sort:\n",
    "        # sort by count\n",
    "        dic = defaultdict(int)\n",
    "        #统计词频\n",
    "        for item in items:\n",
    "            for i in item.split(\" \"):\n",
    "                i = i.strip()\n",
    "                if not i: continue\n",
    "                i = i if not lower else item.lower()\n",
    "                dic[i] += 1\n",
    "        # 排序\n",
    "        dic=sorted(dic.items(),key=lambda x:x[1],reverse=True)\n",
    "        for i, item in enumerate(dic):\n",
    "            key = item[0]\n",
    "            if min_count and min_count > item[1]:\n",
    "                continue\n",
    "            result.append(key)\n",
    "    else:\n",
    "        # sort by items\n",
    "        for i, item in enumerate(items):\n",
    "            item = item if not lower else item.lower()\n",
    "            result.append(item)\n",
    "    vocab=[(v,i) for i,v in enumerate(result)]\n",
    "    reverse_vocab=[(i,v) for v,i in vocab]\n",
    "\n",
    "    return vocab, reverse_vocab\n",
    "\n",
    "#存词库\n",
    "def save_sentences(sentences,path):\n",
    "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for line in sentences:\n",
    "            f.write(\"%s\\n\"%line.strip())\n",
    "    print(\"save sentences path:%s\"%path)\n",
    "\n",
    "def main_():\n",
    "    lines,sentences = read_data('datasets/train_seg_x.txt',\n",
    "                      'datasets/train_seg_y.txt',\n",
    "                      'datasets/test_seg_x.txt')\n",
    "    vocab, reverse_vocab = build_vocab(lines)\n",
    "    save_vocab(vocab,\"datasets/vocab.txt\")\n",
    "    save_sentences(sentences,\"datasets/sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20446700\n",
      "save sentences path:datasets/sentences.txt\n"
     ]
    }
   ],
   "source": [
    "main_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存pickle\n",
    "def dump_pkl(vocab,p_path,overwrite=True):\n",
    "    if p_path and os.path.exists(p_path) and not overwrite:\n",
    "        return\n",
    "    if p_path:\n",
    "        with open(p_path,\"wb\") as f:\n",
    "            pickle.dump(vocab,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"save pkl:%s\"%p_path)\n",
    "\n",
    "#加载pickle\n",
    "def load_pkl(path):\n",
    "    with open(path,\"rb\") as f:\n",
    "        result=pickle.load(f)\n",
    "    return result\n",
    "\n",
    "def build_w2v(out_path, sentence_path,w2v_bin_path=\"w2v.bin\"):\n",
    "    '''\n",
    "    :param out_path: word2vec.txt\n",
    "    :param sentence_path: sentences.txt\n",
    "    :param w2v_bin_path: 模型路径\n",
    "    '''\n",
    "\n",
    "    if w2v_bin_path and os.path.exists(w2v_bin_path):\n",
    "        model=KeyedVectors.load_word2vec_format(w2v_bin_path,binary=True)\n",
    "    else:\n",
    "        sentences=LineSentence(sentence_path)\n",
    "        w2v=Word2Vec(sentences=sentences,size=256,window=5,iter=10,sg=1)\n",
    "        w2v.wv.save_word2vec_format(w2v_bin_path, binary=True)\n",
    "        model=w2v.wv\n",
    "    sim = w2v.wv.similarity('技师', '车主')\n",
    "    print('技师 vs 车主 similarity score:', sim)\n",
    "    word_dict={}\n",
    "    for word in model.vocab:\n",
    "        word_dict[word]=model[word]\n",
    "    dump_pkl(word_dict,out_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "build_w2v(out_path=\"datasets/word2vec.txt\",sentence_path=\"datasets/sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
