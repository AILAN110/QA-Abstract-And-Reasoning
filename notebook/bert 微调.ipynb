{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert 微调\n",
    "### 文本分类\n",
    "    *针对[CLS]后接分类层\n",
    "### 语义匹配\n",
    "    *可当做二分类\n",
    "    *使用孪生网络（暂不作介绍）\n",
    "    \n",
    "### 序列标注\n",
    "    *针对token,外加lstm+crf\n",
    "### QA问答\n",
    "    *训练start、end位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertPreTrainedModel,BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1、文本分类\n",
    "#### 1.1 模型代码\n",
    "#### 1.2 注意事项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert + textcnn\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,filter_sizes):\n",
    "        super(Conv1d,self).__init__()\n",
    "        self.convs=nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel,\n",
    "                     out_channels=out_channel,\n",
    "                     kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])  #每一次conv1d通道可以不同\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        for n in self.convs:\n",
    "            nn.init.xavier_uniform_(m.weight.data)  #初始化\n",
    "            nn.init.constant_(m.bias,data,0.1)  #初始化\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return [F.relu(conv(x)) for conv in self.convs]\n",
    "\n",
    "class BertCNN(BertPreTrainedModel):\n",
    "    def __init__(self,config,num_labels,n_filters,filter_sizes):\n",
    "        super(BertCNN).__init__(config)\n",
    "        self.num_labels=num_labels\n",
    "        self.bert=BertModel(config)\n",
    "        self.dropout=nn.Dropout(config.drop_prob)\n",
    "        #Textcnn\n",
    "        self.convs=Conv1d(config.hidden_size,n_filters,filter_sizes)\n",
    "        self.classifier=nn.Linear(len(filter_sizes)*n_filters,num_labels)\n",
    "        #lstm\n",
    "        #self.rnn = nn.LSTM(config.hidden_size, rnn_hidden_size, num_layers,bidirectional=bidirectional, batch_first=True, dropout=dropout)\n",
    "        #self.classifier = nn.Linear(rnn_hidden_size * 2, num_labels)\n",
    "        \n",
    "        #attention\n",
    "        #self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        #self.W_w = nn.Parameter(torch.Tensor(config.hidden_size, config.hidden_size))\n",
    "        #self.u_w = nn.Parameter(torch.Tensor(config.hidden_size, 1))\n",
    "        #nn.init.uniform_(self.W_w, -0.1, 0.1)\n",
    "        #nn.init.uniform_(self.u_w, -0.1, 0.1)\n",
    "        \n",
    "        self.apply(self.init_bert_weights)\n",
    "    \n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids=None,attention_mask=None,labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: 词对应的 id\n",
    "            token_type_ids: 区分句子，0 为第一句，1表示第二句\n",
    "            attention_mask: 区分 padding 与 token， 1表示是token，0 为padding\n",
    "        \"\"\"\n",
    "        encoded_layers, _ = self.bert(\n",
    "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        # encoded_layers: [batch_size, seq_len, bert_dim=768]\n",
    "        \n",
    "        encoded_layers = self.dropout(encoded_layers)\n",
    "        \"\"\"\n",
    "        Textcnn（3选1）\n",
    "        # 对encoded_layers做维度调整\n",
    "        # 调用conv层\n",
    "        # 图中所示采用最大池化融合\n",
    "        \"\"\"\n",
    "        encoded_layers = encoded_layers.permute(0, 2, 1)\n",
    "        # encoded_layers: [batch_size, bert_dim=768, seq_len]\n",
    "\n",
    "        conved = self.convs(encoded_layers) ##[batch_size,768,seq_len]--nn.Conv()--> n_filters个[batch_size,out_channel,xxx]\n",
    "        # conved 是一个列表， conved[0]: [batch_size, filter_num, *]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)\n",
    "                  for conv in conved]   ##[batch_size,out_channel,1]--->[batch_size,out_channel]  n_filters个\n",
    "        # pooled 是一个列表， pooled[0]: [batch_size, filter_num]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat: [batch_size, filter_num * len(filter_sizes)]\n",
    "        '''\n",
    "        bert+rnn可以替换上面的代码(3选1)\n",
    "         _, (hidden, cell) = self.rnn(encoded_layers)\n",
    "         cat = self.dropout(\n",
    "            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))  # 连接最后一层的双向输出\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        bert_attention可以替换上面的代码（3选1）\n",
    "        score = torch.tanh(torch.matmul(encoded_layers, self.W_w))\n",
    "        # score: [batch_size, seq_len, bert_dim]\n",
    "\n",
    "        attention_weights = F.softmax(torch.matmul(score, self.u_w), dim=1)\n",
    "        # attention_weights: [batch_size, seq_len, 1]\n",
    "\n",
    "        scored_x = encoded_layers * attention_weights\n",
    "        # scored_x : [batch_size, seq_len, bert_dim]\n",
    "\n",
    "        cat = torch.sum(scored_x, dim=1)\n",
    "        # cat: [batch_size, bert_dim=768]\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        logits = self.classifier(cat)\n",
    "        # logits: [batch_size, output_dim]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
