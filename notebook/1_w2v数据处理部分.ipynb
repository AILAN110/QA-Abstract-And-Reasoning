{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Project_1 问答摘要与推理\n",
    "\n",
    "## 1.1 问题描述\n",
    "\n",
    "项目一是由百度AI技术生态部门提供，题目为“汽车大师问答摘要与推理”。\n",
    "\n",
    "要求大家使用汽车大师提供的11万条（技师与用户的多轮对话与诊断建议报告数据）建立模型，模型需基于对话文本、用户问题、车型与车系，输出包含摘要与推断的报告文本，综合考验模型的归纳总结与推断能力。该解决方案可以节省大量人工时间，提高用户获取回答和解决方案的效率。\n",
    "\n",
    "## 1.2 数据说明\n",
    "\n",
    "对于每个用户问题\"QID\"，有对应文本形式的文本集合 D = \"Brand\", \"Collection\", \"Problem\", \"Conversation\"，要求阅读理解系统自动对D进行分析，输出相应的报告文本\"Report\"，其中包含摘要与推理。目标是\"Report\"可以正确、完整、简洁、清晰、连贯地对D中的信息作归纳总结与推理。\n",
    "\n",
    "训练：所提供的训练集（82943条记录）建立模型，基于汽车品牌、车系、问题内容与问答对话的文本，输出建议报告文本\n",
    "\n",
    "输出结果：对所提供的测试集（20000条记录）使用训练好的模型，输出建议报告的结果文件，通过最终测评得到评价分数\n",
    "\n",
    "请提交一个CSV文件，包含QID和Prediction两个字段，分隔符为逗号(',')，请注意区分大小写。参考样例如下：\n",
    "\n",
    "\n",
    "QID\n",
    "\n",
    "Prediction\n",
    "\n",
    "\n",
    "Q103432 你的预测 \n",
    "Q100965 你的预测 \n",
    "\n",
    "训练、测试数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "import pandas as pd\n",
    "train_path = \"week1-homework/datasets/AutoMaster_TrainSet.csv\"\n",
    "test_path = \"week1-homework/datasets/AutoMaster_TestSet.csv\"\n",
    "df = pd.read_csv(train_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82943 entries, 0 to 82942\n",
      "Data columns (total 6 columns):\n",
      "QID         82943 non-null object\n",
      "Brand       81642 non-null object\n",
      "Model       81642 non-null object\n",
      "Question    82943 non-null object\n",
      "Dialogue    82941 non-null object\n",
      "Report      82873 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰GL级</td>\n",
       "      <td>方向机重，助力泵，方向机都换了还是一样</td>\n",
       "      <td>技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了...</td>\n",
       "      <td>随时联系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰M级</td>\n",
       "      <td>奔驰ML500排气凸轮轴调节错误</td>\n",
       "      <td>技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障...</td>\n",
       "      <td>随时联系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>宝马</td>\n",
       "      <td>宝马X1(进口)</td>\n",
       "      <td>2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地换挡位PRND车辆闯...</td>\n",
       "      <td>技师说：你好，4缸自然吸气发动机N46是吧，先挂空档再挂其他档有没有闯动呢，变速箱油液位是否...</td>\n",
       "      <td>行驶没有顿挫的感觉，原地换挡有闯动，刹车踩重没有，这是力的限制的作用，应该没有问题</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>牧马人</td>\n",
       "      <td>3.0V6发动机号在什么位置，有照片最好！</td>\n",
       "      <td>技师说：右侧排气管上方，缸体上靠近变速箱|车主说：[图片]|车主说：是不是这个？|车主说：这...</td>\n",
       "      <td>举起车辆，在左前轮这边的缸体上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>奔驰</td>\n",
       "      <td>奔驰C级</td>\n",
       "      <td>2012款奔驰c180怎么样，维修保养，动力，值得拥有吗</td>\n",
       "      <td>技师说：家庭用车的话，还是可以入手的|技师说：维修保养费用不高|车主说：12年的180市场价...</td>\n",
       "      <td>家庭用车可以入手的，维修保养价格还可以。车况好，价格合理可以入手</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID Brand     Model                                           Question  \\\n",
       "0  Q1    奔驰     奔驰GL级                                方向机重，助力泵，方向机都换了还是一样   \n",
       "1  Q2    奔驰      奔驰M级                                   奔驰ML500排气凸轮轴调节错误   \n",
       "2  Q3    宝马  宝马X1(进口)  2010款宝马X1，2011年出厂，2.0排量，通用6L45变速箱，原地换挡位PRND车辆闯...   \n",
       "3  Q4  Jeep       牧马人                              3.0V6发动机号在什么位置，有照片最好！   \n",
       "4  Q5    奔驰      奔驰C级                       2012款奔驰c180怎么样，维修保养，动力，值得拥有吗   \n",
       "\n",
       "                                            Dialogue  \\\n",
       "0  技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了...   \n",
       "1  技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障...   \n",
       "2  技师说：你好，4缸自然吸气发动机N46是吧，先挂空档再挂其他档有没有闯动呢，变速箱油液位是否...   \n",
       "3  技师说：右侧排气管上方，缸体上靠近变速箱|车主说：[图片]|车主说：是不是这个？|车主说：这...   \n",
       "4  技师说：家庭用车的话，还是可以入手的|技师说：维修保养费用不高|车主说：12年的180市场价...   \n",
       "\n",
       "                                      Report  \n",
       "0                                       随时联系  \n",
       "1                                       随时联系  \n",
       "2  行驶没有顿挫的感觉，原地换挡有闯动，刹车踩重没有，这是力的限制的作用，应该没有问题  \n",
       "3                            举起车辆，在左前轮这边的缸体上  \n",
       "4           家庭用车可以入手的，维修保养价格还可以。车况好，价格合理可以入手  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、分词器介绍\n",
    "分词器的不同，分出的结果也有差别，对模型训练的训练起着一定的影响,下面主要介绍jieba、SnowNlp、pkuseg、THULAC、LAC五种分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、1  jieba\n",
    "•jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型\n",
    "\n",
    "•jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8\n",
    "\n",
    "•jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用\n",
    "\n",
    "•jieba.lcut 以及 jieba.lcut_for_search 直接返回 list\n",
    "\n",
    "•jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode:我/来到/北京/清华/清华大学/华大/大学\n",
      "Default Mode:我/来到/北京/清华大学\n",
      "Default Mode:我/来到/北京/清华大学\n",
      "Search Egine Mode:我/来到/北京/清华/华大/大学/清华大学\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "#全模式\n",
    "seg_text=jieba.cut(\"我来到北京清华大学\",cut_all=True)\n",
    "print('Full Mode:' + '/'.join(seg_text))\n",
    "#精确模式\n",
    "seg_text=jieba.cut(\"我来到北京清华大学\",cut_all=False)\n",
    "print('Default Mode:' + '/'.join(seg_text))\n",
    "#默认模式\n",
    "seg_text=jieba.cut(\"我来到北京清华大学\")\n",
    "print('Default Mode:' + '/'.join(seg_text))\n",
    "\n",
    "#搜索引擎模式\n",
    "seg_text=jieba.cut_for_search(\"我来到北京清华大学\")\n",
    "print('Search Egine Mode:' + '/'.join(seg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词= (我)，词性= (自定义标注)\n",
      "单词= (来到)，词性= (v)\n",
      "单词= (北京)，词性= (ns)\n",
      "单词= (清华大学)，词性= (nt)\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "#带词性的分割，可实现有规则的词性标注功能\n",
    "# jieba.load_userdict(\"filename\")\n",
    "jieba.add_word(\"我\",tag=\"自定义标注\")   #利用结巴完成标注\n",
    "words=pseg.cut(\"我来到北京清华大学\")\n",
    "for w,s in words:\n",
    "    print(\"单词= ({})，词性= ({})\".format(w,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、2  snownlp\n",
    "SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，\n",
    "于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。\n",
    "注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snownlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8f6d57238a05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msnownlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowNLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowNLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"我来到北京清华大学\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'snownlp'"
     ]
    }
   ],
   "source": [
    "from snownlp import SnowNLP \n",
    "s = SnowNLP(\"我来到北京清华大学\")\n",
    "print(' '.join(s.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、3 pkuseg\n",
    "pkuseg具有如下几个特点：\n",
    "\n",
    "1.多领域分词。不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了新闻领域，网络领域，医药领域，旅游领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。各领域分词样例可参考 example.txt。\n",
    "\n",
    "2.更高的分词准确率。相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。\n",
    "\n",
    "3.支持用户自训练模型。支持用户使用全新的标注数据进行训练。\n",
    "\n",
    "4.支持词性标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pkuseg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-23aec3b289a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpkuseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpku_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkuseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpkuseg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpku_seg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'我来到北京清华大学'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pkuseg'"
     ]
    }
   ],
   "source": [
    "import pkuseg\n",
    "pku_seg = pkuseg.pkuseg()\n",
    "print(' '.join(pku_seg.cut('我来到北京清华大学')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、4 THULAC\n",
    "THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：\n",
    "\n",
    "1.能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。\n",
    "\n",
    "2.准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。\n",
    "\n",
    "3.速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "thu_lac = thulac.thulac(seg_only=True) \n",
    "thu_result = thu_lac.cut(\"我来到北京清华大学\", text=True) \n",
    "print(thu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、5 LAC\n",
    "LAC全称Lexical Analysis of Chinese，是百度自然语言处理部研发的一款联合的词法分析工具，实现中文分词、词性标注、专名识别等功能。该工具具有以下特点与优势：\n",
    "\n",
    "- 效果好：通过深度学习模型联合学习分词、词性标注、专名识别任务，整体效果F1值超过0.91，词性标注F1值超过0.94，专名识别F1值超过0.85，效果业内领先。\n",
    "- 效率高：精简模型参数，结合Paddle预测库的性能优化，CPU单线程性能达800QPS，效率业内领先。\n",
    "- 可定制：实现简单可控的干预机制，精准匹配用户词典对模型进行干预。词典支持长片段形式，使得干预更为精准。\n",
    "- 调用便捷：支持一键安装，同时提供了Python、Java和C++调用接口与调用示例，实现快速调用和集成。\n",
    "- 支持移动端: 定制超轻量级模型，体积仅为2M，主流千元手机单线程性能达200QPS，满足大多数移动端应用的需求，同等体积量级效果业内领先。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'LAC'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-940bcf440f14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mLAC\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLAC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLAC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'seg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu\"我来到北京清华大学\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu\"LAC是个优秀的分词工具\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu\"百度是一家高科技公司\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mseg_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'LAC'"
     ]
    }
   ],
   "source": [
    "from LAC import LAC\n",
    "lac = LAC(mode='seg')\n",
    "text = u\"我来到北京清华大学\"\n",
    "texts=[u\"LAC是个优秀的分词工具\", u\"百度是一家高科技公司\"]\n",
    "seg_result = lac.run(texts)\n",
    "print(seg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、 w2v训练\n",
    "采用gensim来进行词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、1 加载数据--》新建模型--》模型训练--》模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_path=\"week1-homework\\datasets\\sentences.txt\"    #加载数据：所有语料的分词，一行一个句子的分词\n",
    "# sentences=[line.strip().split() for line in sentences]   #不建议使用，占大量内存\n",
    "sentences=LineSentence(sentence_path)    #返回是迭代器格式\n",
    "#模型建立&训练：size=向量长度，sg=1（skip-gram）,window=窗口大小,iter=迭代次数\n",
    "w2v=Word2Vec(sentences=sentences,size=256,sg=1,window=5,iter=20)   \n",
    "\n",
    "#保存词向量\n",
    "word_dict={}\n",
    "for word in w2v.wv.vocab:\n",
    "    word_dict[word] = w2v[word]\n",
    "print(word_dict)\n",
    "#--------->保存词向量方法pkl\n",
    "#保存模型\n",
    "#方式一：KeyedVectors实例的形式保存，不是模型的完整状态，不可再次进行训练\n",
    "w2v.wv.save_word2vec_format(\"xxx.bin\", binary=True)\n",
    "#方式二：完整保存\n",
    "w2v.save(\"xxx.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、2 加载模型----》再次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13450318, 20446635)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_path=\"week1-homework\\datasets\\sentences.txt\"    #加载数据：所有语料的分词，一行一个句子的分词\n",
    "# sentences=[line.strip().split() for line in sentences]   #不建议使用，占大量内存\n",
    "sentences=LineSentence(sentence_path)    #返回是迭代器格式\n",
    "w2v=Word2Vec.load(\"xxx.model\")\n",
    "w2v.train(sentences,total_examples=1,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、3 加载模型--》。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, numpy.float32 found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ae588bf16c14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mword_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mword_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, numpy.float32 found"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"xxx.bin\", binary=True)\n",
    "word_dict = {}\n",
    "for word in model.vocab:\n",
    "    word_dict[word] = model[word]\n",
    "word_dict   #存入文本可以转格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、4 annoy来实现most_similar，加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def timeit(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        res = f(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(\"%s函数运行时间为：%.8f\" %(f.__name__, end_time - start_time))\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奖赏\n",
      "客气\"\n",
      "886\n",
      "好勒\n",
      "太棒了\n",
      "明儿\n",
      "随时欢迎\n",
      "生意兴隆\n",
      "祝你好运\n",
      "大吉\n",
      "夸奖\n",
      "run函数运行时间为：1.27857852\n"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def run():\n",
    "    from collections import OrderedDict\n",
    "    from annoy import AnnoyIndex\n",
    "    word_index = OrderedDict()   #word id映射\n",
    "    for counter, key in enumerate(model.vocab.keys()):\n",
    "        word_index[key] = counter\n",
    "\n",
    "    wv_index = AnnoyIndex(256)   #向量size\n",
    "    i = 0\n",
    "    for key in model.vocab.keys():    #任意词向量均可\n",
    "        v = model[key]\n",
    "        wv_index.add_item(i,v)    #id 向量 映射表,v为list或ndarray\n",
    "        i += 1\n",
    "    wv_index.build(10)  #构建树\n",
    "    # wv_index.save('wv_index_build10.index')   #保存结构\n",
    "    # wv_index=AnnoyIndex(256)\n",
    "    # wv_index.load('')           #模型加载\n",
    "\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    for item in wv_index.get_nns_by_item(word_index[u'奖赏'], 11):    #word_id,向量的11个最相似\n",
    "        print(reverse_word_index[item])\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
