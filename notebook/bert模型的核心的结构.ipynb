{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertEncode模型分解\n",
    "* 语言：pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、embedding 层\n",
    "#### nn.Embedding\n",
    "    \n",
    "    ** 参数：vocab_size,向量长度,padding_idx（填充id）\n",
    "    \n",
    "#### nn.LayerNorm(对-1维进行正则化)\n",
    "\n",
    "    ** 参数：hidden_size（正则化的长度）、eps=平滑参数\n",
    "    \n",
    "#### nn.Dropout\n",
    "    ** 参数：丢弃率\n",
    "\n",
    "#### self.register_buffer(\"a\",aa)\n",
    "    ** 参数：变量名、aa(内容)\n",
    "    ** 作用：注册一个变量a,值为aa,赋给self对象\n",
    " \n",
    "#### torch.expand((2,2))\n",
    "    ** 参数：shape（元组）\n",
    "    ** 功能：将源数据，广播到指定维度\n",
    "    \n",
    "#### tensor.permute(1,0,2)\n",
    "    ** 参数：维度的顺序\n",
    "    ** 功能：转置\n",
    " \n",
    "#### tensor.contiguous()\n",
    "    ** 参数：无\n",
    "    ** 功能：保证tensor在内存中的存储是连续的\n",
    "    ** 备注：view()是建立在contiguous的变量之上，在使用transpose、permute之后，必须加一个contiguous,才能使用view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.token_embedding=nn.Embedding(config.vocab_size,config.hidden_size,padding_idx=config.pad_token_id)  #初始化向量矩阵\n",
    "        self.position_embdedding=nn.Embedding(config.max_position_embedding,config.hidden_size)  #单个字映射\n",
    "        self.segment_embedding=nn.Embedding(3,config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "        self.dropout=nn.Dropout(config.dropout_prob)\n",
    "        \n",
    "        self.register_buffer(\"position_ids\",torch.arange(config.max_position_embedding).expand((1,-1)))   #[1,max_position_embedding]\n",
    "    \n",
    "    def forward(self,seq,labels):\n",
    "        x=self.token_embedding(seq)+self.position_embdedding(self.position_ids[:,:seq.size(1)])+self.segment_embedding(labels)\n",
    "        return self.dropout(self.LayerNorm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、Encode 层\n",
    "#### nn.ModuleList\n",
    "    * 参数：list（与list无异）\n",
    "#### nn.Linear\n",
    "    * 参数：上一层size、下一层size\n",
    "    * 功能：矩阵乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncode(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.layers=nn.ModuleList([BertLayer(config) for _ in range(config.layer_nums)])    #n个模块\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward=config.chunk_size_feed_forward\n",
    "        self.seq_len_dim=1\n",
    "        self.is_decoder=config.is_decoder\n",
    "        self.attention=BertAttention(config)   #attetion+ add&norm\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:  #decoder在加一个attention\n",
    "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate=BertIntermediate(config)   #feed forward\n",
    "        self.output=BertOutput(config)    #add & norm\n",
    "    \n",
    "    def forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,output_attention=False):\n",
    "        self_attention_outputs=self.attention(hidden_states,attention_mask,head_mask,output_attention=output_attention)\n",
    "        attention_output=self_attention_outputs[0]\n",
    "        outputs=self_attention_outputs[1:]\n",
    "        \n",
    "        \"\"\"\n",
    "            注释掉的这段代码在BERT里是不用的，在一些用于生成式任务的预训练模型会使用，其实这个地方是能体现出BERT和GPT的不同。\n",
    "            if self.is_decoder and encoder_hidden_states is not None:\n",
    "                assert hasattr(\n",
    "                    self, \"crossattention\"\n",
    "                ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                cross_attention_outputs = self.crossattention(\n",
    "                    attention_output,\n",
    "                    attention_mask,\n",
    "                    head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "                attention_output = cross_attention_outputs[0]\n",
    "                outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "        \"\"\"\n",
    "        # 这段代码里的chunking部分不是给BERT用的，但是源码把BertIntermediate、BertOutput都封装到里面了，所以我们直接看feed_forward_chunk这个函数就可以了\n",
    "        #  layer_output = apply_chunking_to_forward(\n",
    "        #  self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        #  )\n",
    "        layer_output=self.feed_forward_chunk(attention_output)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        # BertIntermediate，结构见图\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        # BertOutput，结构见图\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention 内部\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.self_attention=BertSelfAttention(config)\n",
    "        self.output=BertSelfOutput(config)\n",
    "        self.pruned_heads=set()\n",
    "    \n",
    "    def forward(self,hidden_states,attention_mask=None,head_mask=None,\n",
    "                encoder_hidden_states=None,encoder_attention_mask=None,output_attention=False):\n",
    "        attention_output,attention_prob=self.self_attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            output_attention\n",
    "        )\n",
    "        return self.output(attention_output,hidden_states),attention_prob\n",
    "\n",
    "#attention部分\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size%config.num_attention_heads==0:\n",
    "            raise Exception(\"不能整除\")\n",
    "        self.num_attention_heads=config.num_attention_heads\n",
    "        self.attention_head_size=config.hidden_size//config.num_attention_heads\n",
    "        self.all_head_size=self.num_attention_heads*self.attention_head_size\n",
    "        \n",
    "        #输入线性转换\n",
    "        self.query=nn.Linear(config.hidden_size,self.all_head_size)\n",
    "        self.value=nn.Linear(config.hidden_size,self.all_head_size)\n",
    "        self.key=nn.Linear(config.hidden_size,self.all_head_size)\n",
    "        \n",
    "        self.normalize=nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.dropout=nn.Dropout(config.dropout_prob)\n",
    "    \n",
    "    #切头\n",
    "    def transpose_for_scores(self,x):\n",
    "        x_shape=x.size()[:1]+(self.num_attention_heads,self.attention_head_size)\n",
    "        x=x.view(*x_shape)\n",
    "        return x.permute(0,2,1,3)     #[bs,num_haeds,seq_len,attn_size]\n",
    "    \n",
    "    def forward(self,hidden_states,attention_mask=None,head_mask=None,\n",
    "                encoder_hidden_states=None,encoder_attention_mask=None,output_attention=False):\n",
    "        all_query=self.query(hidden_states)\n",
    "        if encoder_hidden_states is not None:   #decode\n",
    "            all_key=self.key(encoder_hidden_states)\n",
    "            all_value=self.value(encoder_hidden_states)\n",
    "            attention_mask=encoder_attention_mask\n",
    "        else:\n",
    "            all_key=self.key(hidden_states)\n",
    "            all_value=self.value(hidden_states)\n",
    "        #切头\n",
    "        query_layer=self.transpose_for_scores(all_query)\n",
    "        key_layer=self.transpose_for_scores(all_key)\n",
    "        value_layer=self.transpose_for_scores(all_value)\n",
    "        \n",
    "        #自注意力\n",
    "        key_layer=key_layer.permute(0,1,3,2)\n",
    "        d_k=query_layer.size(-1)\n",
    "        attetion_scores=torch.matmul(query_layer,key_layer)/math.sqrt(d_k)\n",
    "        \n",
    "        #mask\n",
    "        if attention_mask is not None:\n",
    "            attetion_scores+=attention_mask\n",
    "        \n",
    "        #归一化\n",
    "        attention_probs=self.normalize(attetion_scores)\n",
    "        \n",
    "        #head_mask:可以有选择的mask多个头\n",
    "        if head_mask is not None:\n",
    "            attention_probs=attention_probs*head_mask\n",
    "        \n",
    "        context_layer=torch.matmul(attention_probs,value_layer)   #下一次输入\n",
    "        #合并头\n",
    "        context_layer=context_layer.permute(0,2,1,3).contiguous()\n",
    "        context_shape=context_layer.size()[:-2]+(self.all_head_size,)\n",
    "        context_layer=context_layer.view(*context_shape)\n",
    "        \n",
    "        outputs=(context_layer,attention_probs) if output_attention else (context_layer,)\n",
    "        return outputs\n",
    "        \n",
    "#add & norm\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.dense=nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.dropout=nn.Dropout(config.dropout_prob)\n",
    "        self.layer_norm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "    \n",
    "    def forward(self,attention_outputs,hidden_state):\n",
    "        attention_outputs=self.dense(attention_outputs)\n",
    "        attention_outputs=self.dropout(attention_outputs)\n",
    "        return self.layer_norm(attention_outputs+hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全连接+激活函数\n",
    "ACT2FN={\"relu\":nn.ReLU}\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense=nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        if isinstance(config.hidden_act,str):\n",
    "            self.intermediate_act_fn=ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn=config.hidden_act\n",
    "    \n",
    "    def forward(self,hidden_states):\n",
    "        hidden_states=self.dense(hidden_states)\n",
    "        hidden_states=self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add & norm\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.dense=nn.Linear(config.intermediate_size,config.hidden_size)\n",
    "        self.layer_norm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "        self.dropout=nn.dropout(config.dropout_prob)\n",
    "    \n",
    "    def forward(self,hidden_states,input_tensor):\n",
    "        hidden_states=self.dense(hidden_states)\n",
    "        hidden_states=self.dropout(hidden_states)\n",
    "        return self.layer_norm(input_tensor+hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类层 \n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense=nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self,hidden_states):\n",
    "        cls_hidden=hidden_states[:,0]\n",
    "        pooled_output=self.dense(cls_hidden)\n",
    "        pooled_output=self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
