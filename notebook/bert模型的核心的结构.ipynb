{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertEncode模型分解\n",
    "* 语言：pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、embedding 层\n",
    "#### nn.Embedding\n",
    "    \n",
    "    ** 参数：vocab_size,向量长度,padding_idx（填充id）\n",
    "    \n",
    "#### nn.LayerNorm(对-1维进行正则化)\n",
    "\n",
    "    ** 参数：hidden_size（正则化的长度）、eps=平滑参数\n",
    "    \n",
    "#### nn.Dropout\n",
    "    ** 参数：丢弃率\n",
    "\n",
    "#### self.register_buffer(\"a\",aa)\n",
    "    ** 参数：变量名、aa(内容)\n",
    "    ** 作用：注册一个变量a,值为aa,赋给self对象\n",
    " \n",
    "#### torch.expand((2,2))\n",
    "    ** 参数：shape（元组）\n",
    "    ** 功能：将源数据，广播到指定维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.token_embedding=nn.Embedding(config.vocab_size,config.hidden_size,padding_idx=config.pad_token_id)  #初始化向量矩阵\n",
    "        self.position_embdedding=nn.Embedding(config.max_position_embedding,config.hidden_size)  #单个字映射\n",
    "        self.segment_embedding=nn.Embedding(3,config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "        self.dropout=nn.Dropout(config.dropout_prob)\n",
    "        \n",
    "        self.register_buffer(\"position_ids\",torch.arange(config.max_position_embedding).expand((1,-1)))   #[1,max_position_embedding]\n",
    "    \n",
    "    def forward(self,seq,labels):\n",
    "        x=self.token_embedding(seq)+self.position_embdedding(self.position_ids[:,:seq.size(1)])+self.segment_embedding(labels)\n",
    "        return self.dropout(self.LayerNorm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、Encode 层\n",
    "#### nn.ModuleList\n",
    "    * 参数：list（与list无异）\n",
    "#### nn.Linear\n",
    "    * 参数：上一层size、下一层size\n",
    "    * 功能：矩阵乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncode(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.layers=nn.ModuleList([BertLayer(config) for _ in range(config.layer_nums)])    #n个模块\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward=config.chunk_size_feed_forward\n",
    "        self.seq_len_dim=1\n",
    "        self.is_decoder=config.is_decoder\n",
    "        self.attention=BertAttention(config)   #attetion+ add&norm\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:  #decoder在加一个attention\n",
    "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate=BertIntermediate(config)   #feed forward\n",
    "        self.output=BertOutput(config)    #add & norm\n",
    "    \n",
    "    def forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,output_attention=False):\n",
    "        self_attention_outputs=self.attention(hidden_states,attention_mask,head_mask,output_attention)\n",
    "        attention_output=self_attention_outputs[0]\n",
    "        outputs=self_attention_outputs[1:]\n",
    "        \n",
    "        \"\"\"\n",
    "            注释掉的这段代码在BERT里是不用的，在一些用于生成式任务的预训练模型会使用，其实这个地方是能体现出BERT和GPT的不同。\n",
    "            if self.is_decoder and encoder_hidden_states is not None:\n",
    "                assert hasattr(\n",
    "                    self, \"crossattention\"\n",
    "                ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                cross_attention_outputs = self.crossattention(\n",
    "                    attention_output,\n",
    "                    attention_mask,\n",
    "                    head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "                attention_output = cross_attention_outputs[0]\n",
    "                outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "        \"\"\"\n",
    "        # 这段代码里的chunking部分不是给BERT用的，但是源码把BertIntermediate、BertOutput都封装到里面了，所以我们直接看feed_forward_chunk这个函数就可以了\n",
    "        #  layer_output = apply_chunking_to_forward(\n",
    "        #  self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        #  )\n",
    "        layer_output=self.feed_forward_chunk(attention_output)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        # BertIntermediate，结构见图\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        # BertOutput，结构见图\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention 内部\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全连接+激活函数\n",
    "ACT2FN={\"relu\":nn.ReLU}\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense=nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        if isinstance(config.hidden_act,str):\n",
    "            self.intermediate_act_fn=ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn=config.hidden_act\n",
    "    \n",
    "    def forward(self,hidden_states):\n",
    "        hidden_states=self.dense(hidden_states)\n",
    "        hidden_states=self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add & norm\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.dense=nn.Linear(config.intermediate_size,config.hidden_size)\n",
    "        self.layer_norm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "        self.dropout=nn.dropout(config.dropout_prob)\n",
    "    \n",
    "    def forward(self,hidden_states,input_tensor):\n",
    "        hidden_states=self.dense(hidden_states)\n",
    "        hidden_states=self.dropout(hidden_states)\n",
    "        return self.layer_norm(input_tensor+hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类层 \n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense=nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self,hidden_states):\n",
    "        cls_hidden=hidden_states[:,0]\n",
    "        pooled_output=self.dense(cls_hidden)\n",
    "        pooled_output=self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
