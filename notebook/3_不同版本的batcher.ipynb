{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练之前，进行的batch操作\n",
    "### 1、tensorflow版本的dataset\n",
    "\n",
    "### 2、pytorch版本的dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenier/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "START_DECODING = '[START]'\n",
    "STOP_DECODING = '[STOP]'\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self,vocab_path,max_size):\n",
    "        self.word2id={UNKNOWN_TOKEN:0,PAD_TOKEN:1,START_DECODING:2,STOP_DECODING:3}\n",
    "        self.id2word={0:UNKNOWN_TOKEN,1:PAD_TOKEN,2:START_DECODING,3:STOP_DECODING}\n",
    "        self.count=4\n",
    "        \n",
    "        with open(vocab_path,\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                pair=line.split()\n",
    "                if len(pair)!=2:\n",
    "                    print(\"Warning:the error line is :{}\"%line)\n",
    "                w=pair[0]\n",
    "                if w in [SENTENCE_START,SENTENCE_END,PAD_TOKEN,UNKNOWN_TOKEN,START_DECODING,STOP_DECODING]:\n",
    "                    raise Exception(\"word <%s> is exception\"%w)\n",
    "                if w in self.word2id:\n",
    "                    raise Exception(\"word <%s> is repetition\")\n",
    "                \n",
    "                if max_size>=self.count:\n",
    "                    self.word2id[w]=self.count\n",
    "                    self.id2word[self.count]=w\n",
    "                    self.count+=1\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "    def word_to_id(self,word):\n",
    "        if w not in self.word2id:\n",
    "            return self.word2id[UNKNOWN_TOKEN]\n",
    "        return self.word2id[word]\n",
    "    \n",
    "    def id_to_word(self,w_id):\n",
    "        if w_id not in self.id2word:\n",
    "            raise Exception(\"id %s can not found\"%w_id)\n",
    "        return self.id2word[w_id]\n",
    "    \n",
    "    def size(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、tf版本1\n",
    "#### 读取数据：\n",
    "    * tf.data.TextLineDataset(txt_path)----->将一句输入 整合 指定格式输出  \n",
    "    * tf.data.Dataset.zip((dataset_train_x, dataset_train_y))   #当作zip使用\n",
    "    * train_dataset.shuffle(1000, reshuffle_each_iteration=True).repeat()   #shuffle+repeat()，buffer_size=1000\n",
    "#### 将迭代器处理成dataset:\n",
    "    * tf.data.Dataset.from_generator(generator,output_types={key:tf.int32....},output_shape={key:shape...})   #generator必须是迭代器\n",
    "    为什么要再用这个？\n",
    "    答：因为上面读取数据后，还要进行train、test、eval不同状况下的处理，返回的数据不是dataset类型了，而是字典。。。。\n",
    "#### 构建batch数据,并设置默认填充：\n",
    "    * dataset.padded_batch(batch_size,padded_shapes={key:shape...},padding_values={key:value...},drop_reminder=true)\n",
    "    drop_reminder:表示最后一个不满足的batch是否丢弃\n",
    "#### map操作：\n",
    "    * 对dataset的所有输出进行格式或者映射(map)的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self,vocab,train_x_path,train_y_path,test_x_path,eval_x_path,eval_y_path,max_enc_len, max_dec_len):\n",
    "        self.vocab=vocab\n",
    "        self.train_x_path=train_x_path\n",
    "        self.train_y_path=train_y_path\n",
    "        self.test_x_path=test_x_path\n",
    "        self.eval_x_path=eval_x_path\n",
    "        self.eval_y_path=eval_y_path\n",
    "        self.max_enc_len=max_enc_len\n",
    "        self.max_dec_len=max_dec_len\n",
    "    \n",
    "    #加载数据+处理数据\n",
    "    def example_generator(self,mode,batch_size):\n",
    "        if mode==\"train\":\n",
    "            dataset_train_x=tf.data.TextLineDataset(self.train_x_path)   #读取文件\n",
    "            dataset_train_y=tf.data.TextLineDataset(self.train_y_path)\n",
    "            data_train=tf.data.Dataset.zip((dataset_train_x, dataset_train_y))  #zip x,y\n",
    "            data_train=data_train.shuffle(1000,reshuffle_each_iteration=True).repeat() #打乱\n",
    "            for train_d in data_train:\n",
    "                #可对每一行的enc_x,dec_y进行处理\n",
    "                x,y=train_d\n",
    "                x=x.numpy().decode(\"utf-8\")  #转numpy-->str\n",
    "                y=y.numpy().decode(\"utf-8\")\n",
    "                enc_x=x.split()[:self.max_enc_len]   #分词,限制长度处理\n",
    "                dec_x=y.split()[:self.max_dec_len]   #分词，限制长度处理\n",
    "                enc_len=len(enc_x)\n",
    "                dec_len=len(dec_x)\n",
    "                enc_x=enc_x+[0]*(self.max_enc_len-enc_len)   #填充\n",
    "                enc_x=[self.vocab.word_to_id(w) for w in enc_x]  #转id\n",
    "                start_id=self.vocab.word_to_id(START_DECODING)\n",
    "                stop_id=self.vocab.word_to_id(STOP_DECODING)\n",
    "                dec_input,dec_outputs=get_dec_inp(dec_x,start_id,stop_id)\n",
    "                abstract_sentences=[\"\"]   #预测用到\n",
    "                output = {\n",
    "                \"enc_len\": enc_len,\n",
    "                \"enc_input\": enc_x,\n",
    "                \"dec_input\": dec_input,\n",
    "                \"target\": dec_outputs,\n",
    "                \"dec_len\": dec_len,\n",
    "                \"article\": x,\n",
    "                \"abstract\": y,\n",
    "                \"abstract_sents\": abstract_sentences\n",
    "                }\n",
    "                yield output\n",
    "                \n",
    "            \n",
    "        elif mode==\"test\":\n",
    "            dataset_test_x=tf.data.TextLineDataset(self.test_x_path)\n",
    "            for test_d in dataset_test_x:\n",
    "                #可对每一行的enc_x进行处理\n",
    "                x=test_d.numpy().decode(\"utf-8\")\n",
    "                enc_x=x.split()[:self.max_enc_len]   #分词,限制长度处理\n",
    "                enc_len=len(enc_x)\n",
    "                enc_x=enc_x+[0]*(self.max_enc_len-enc_len)   #填充\n",
    "                enc_x=[self.vocab.word_to_id(w) for w in enc_x]  #转id\n",
    "                abstract_sentences=[]   #预测用到\n",
    "                output = {\n",
    "                \"enc_len\": enc_len,\n",
    "                \"enc_input\": enc_x,\n",
    "                \"dec_input\": [],\n",
    "                \"target\": [],\n",
    "                \"dec_len\": self.max_dec_len,\n",
    "                \"article\": x,\n",
    "                \"abstract\": '',\n",
    "                \"abstract_sents\": abstract_sentences\n",
    "                }\n",
    "                yield output\n",
    "                \n",
    "        else:\n",
    "            dataset_eval_x=tf.data.TextLineDataset(self.eval_x_path)\n",
    "            dataset_eval_y=tf.data.TextLineDataset(self.eval_y_path)\n",
    "            data_evval=tf.data.Dataset.zip((dataset_eval_x,dataset_eval_y))\n",
    "            for eval_d in data_evval:\n",
    "                #可对每一行的enc_x,dec_y进行处理\n",
    "                x,y=train_d\n",
    "                x=x.numpy().decode(\"utf-8\")  #转numpy-->str\n",
    "                y=y.numpy().decode(\"utf-8\")\n",
    "                enc_x=x.split()[:self.max_enc_len]   #分词,限制长度处理\n",
    "                enc_len=len(enc_x)\n",
    "                enc_x=enc_x+[0]*(self.max_enc_len-enc_len)   #填充\n",
    "                enc_x=[self.vocab.word_to_id(w) for w in enc_x]  #转id\n",
    "                abstract_sentences=[]   #预测用到\n",
    "                output = {\n",
    "                \"enc_len\": enc_len,\n",
    "                \"enc_input\": enc_x,\n",
    "                \"dec_input\": [],\n",
    "                \"target\": [],\n",
    "                \"dec_len\": self.max_dec_len,\n",
    "                \"article\": x,\n",
    "                \"abstract\": y,\n",
    "                \"abstract_sents\": abstract_sentences\n",
    "                }\n",
    "                yield output\n",
    "        \n",
    "        \n",
    "    def batch_generator(self,batch_size, mode):\n",
    "        dataset=tf.data.Dataset.from_generator(\n",
    "            lambda:self.example_generator(mode,batch_size),\n",
    "            output_types={\n",
    "                \"enc_len\": tf.int32,\n",
    "                \"enc_input\": tf.int32,\n",
    "                \"dec_input\": tf.int32,\n",
    "                \"target\": tf.int32,\n",
    "                \"dec_len\": tf.int32,\n",
    "                \"article\": tf.string,\n",
    "                \"abstract\": tf.string,\n",
    "                \"abstract_sents\": tf.string\n",
    "                },\n",
    "            output_shape={\n",
    "                \"enc_len\": [],\n",
    "                \"enc_input\":[None],\n",
    "                \"dec_input\": [None],\n",
    "                \"target\": [None],\n",
    "                \"dec_len\": [],\n",
    "                \"article\": [],\n",
    "                \"abstract\": [],\n",
    "                \"abstract_sents\":[None]\n",
    "                }\n",
    "        )\n",
    "        dataset=dataset.padded_batch(\n",
    "            batch_size,\n",
    "            padded_shapes={\n",
    "                \"enc_len\": [],\n",
    "                \"enc_input\":[None],\n",
    "                \"dec_input\": [self.max_dec_len],\n",
    "                \"target\": [self.max_dec_len],\n",
    "                \"dec_len\": [],\n",
    "                \"article\": [],\n",
    "                \"abstract\": [],\n",
    "                \"abstract_sents\":[None]\n",
    "            },\n",
    "            padding_values={\n",
    "                \"enc_len\": -1,\n",
    "                \"enc_input\":1,\n",
    "                \"dec_input\":1 ,\n",
    "                \"target\": 1,\n",
    "                \"dec_len\": -1,\n",
    "                \"article\": b'',\n",
    "                \"abstract\": b'',\n",
    "                \"abstract_sents\":b''\n",
    "            },\n",
    "            drop_reminder=True\n",
    "        )\n",
    "        \n",
    "        def update(entry):\n",
    "            return (\n",
    "                {\n",
    "                \"enc_len\": entry[\"enc_len\"],\n",
    "                \"enc_input\":entry[\"enc_input\"],\n",
    "                \"article\": entry[\"article\"]\n",
    "                },\n",
    "                {\n",
    "                \"dec_input\":entry[\"dec_input\"],\n",
    "                \"target\":entry[\"target\"],\n",
    "                \"dec_len\":entry[\"dec_len\"],\n",
    "                \"abstract\":entry[\"abstract\"],\n",
    "                \"abstract_sents\":entry[\"abstract_sents\"]\n",
    "                }\n",
    "            )\n",
    "        dataset=dataset.map(update)\n",
    "        return dataset\n",
    "    \n",
    "    def batcher(self,params):\n",
    "        return self.batch_generator(params[\"batch_size\"],params[\"mode\"])\n",
    "    \n",
    "    #dec文本进行处理\n",
    "    def get_dec_inp(self,seq,start_id,stop_id):\n",
    "        seq_id=[self.vocab.word_to_id(w) for w in seq]\n",
    "        dec_inp=[start_id]+seq_id\n",
    "        dec_inp=dec_inp[:self.max_dec_len]\n",
    "        dec_out=dec_inp[1:]+[stop_id]\n",
    "        assert len(dec_inp)==len(dec_out)\n",
    "        return dec_inp,dec_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf - tfrecord方式\n",
    "偷个懒，后期更新。。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
