{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练之前，进行的batch操作\n",
    "### 1、tensorflow版本的dataset\n",
    "\n",
    "### 2、pytorch版本的dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenier/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "START_DECODING = '[START]'\n",
    "STOP_DECODING = '[STOP]'\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self,vocab_path,max_size):\n",
    "        self.word2id={UNKNOWN_TOKEN:0,PAD_TOKEN:1,START_DECODING:2,STOP_DECODING:3}\n",
    "        self.id2word={0:UNKNOWN_TOKEN,1:PAD_TOKEN,2:START_DECODING,3:STOP_DECODING}\n",
    "        self.count=4\n",
    "        \n",
    "        with open(vocab_path,\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                pair=line.split()\n",
    "                if len(pair)!=2:\n",
    "                    print(\"Warning:the error line is :{}\"%line)\n",
    "                w=pair[0]\n",
    "                if w in [SENTENCE_START,SENTENCE_END,PAD_TOKEN,UNKNOWN_TOKEN,START_DECODING,STOP_DECODING]:\n",
    "                    raise Exception(\"word <%s> is exception\"%w)\n",
    "                if w in self.word2id:\n",
    "                    raise Exception(\"word <%s> is repetition\")\n",
    "                \n",
    "                if max_size>=self.count:\n",
    "                    self.word2id[w]=self.count\n",
    "                    self.id2word[self.count]=w\n",
    "                    self.count+=1\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "    def word_to_id(self,word):\n",
    "        if w not in self.word2id:\n",
    "            return self.word2id[UNKNOWN_TOKEN]\n",
    "        return self.word2id[word]\n",
    "    \n",
    "    def id_to_word(self,w_id):\n",
    "        if w_id not in self.id2word:\n",
    "            raise Exception(\"id %s can not found\"%w_id)\n",
    "        return self.id2word[w_id]\n",
    "    \n",
    "    def size(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、tf版本1\n",
    "#### 读取数据：\n",
    "    * tf.data.TextLineDataset(txt_path)----->将一句输入 整合 指定格式输出  \n",
    "    * tf.data.Dataset.zip((dataset_train_x, dataset_train_y))   #当作zip使用\n",
    "    * train_dataset.shuffle(1000, reshuffle_each_iteration=True).repeat()   #shuffle+repeat()，buffer_size=1000\n",
    "#### 将迭代器处理成dataset:\n",
    "    * tf.data.Dataset.from_generator(generator,output_types={key:tf.int32....},output_shape={key:shape...})   #generator必须是迭代器\n",
    "    为什么要再用这个？\n",
    "    答：因为上面读取数据后，还要进行train、test、eval不同状况下的处理，返回的数据不是dataset类型了，而是字典。。。。\n",
    "#### 构建batch数据,并设置默认填充：\n",
    "    * dataset.padded_batch(batch_size,padded_shapes={key:shape...},padding_values={key:value...},drop_reminder=true)\n",
    "    drop_reminder:表示最后一个不满足的batch是否丢弃\n",
    "#### map操作：\n",
    "    * 对dataset的所有输出进行格式或者映射(map)的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self,vocab,train_x_path,train_y_path,test_x_path,eval_x_path,eval_y_path,max_enc_len, max_dec_len):\n",
    "        self.vocab=vocab\n",
    "        self.train_x_path=train_x_path\n",
    "        self.train_y_path=train_y_path\n",
    "        self.test_x_path=test_x_path\n",
    "        self.eval_x_path=eval_x_path\n",
    "        self.eval_y_path=eval_y_path\n",
    "        self.max_enc_len=max_enc_len\n",
    "        self.max_dec_len=max_dec_len\n",
    "    \n",
    "    #加载数据+处理数据\n",
    "    def example_generator(self,mode,batch_size):\n",
    "        if mode==\"train\":\n",
    "            dataset_train_x=tf.data.TextLineDataset(self.train_x_path)   #读取文件\n",
    "            dataset_train_y=tf.data.TextLineDataset(self.train_y_path)\n",
    "            data_train=tf.data.Dataset.zip((dataset_train_x, dataset_train_y))  #zip x,y\n",
    "            data_train=data_train.shuffle(1000,reshuffle_each_iteration=True).repeat() #打乱\n",
    "            for train_d in data_train:\n",
    "                #可对每一行的enc_x,dec_y进行处理\n",
    "                x,y=train_d\n",
    "                x=x.numpy().decode(\"utf-8\")  #转numpy-->str\n",
    "                y=y.numpy().decode(\"utf-8\")\n",
    "                enc_x=x.split()[:self.max_enc_len]   #分词,限制长度处理\n",
    "                dec_x=y.split()[:self.max_dec_len]   #分词，限制长度处理\n",
    "                enc_len=len(enc_x)\n",
    "                dec_len=len(dec_x)\n",
    "                enc_x=enc_x+[0]*(self.max_enc_len-enc_len)   #填充\n",
    "                enc_x=[self.vocab.word_to_id(w) for w in enc_x]  #转id\n",
    "                start_id=self.vocab.word_to_id(START_DECODING)\n",
    "                stop_id=self.vocab.word_to_id(STOP_DECODING)\n",
    "                dec_input,dec_outputs=get_dec_inp(dec_x,start_id,stop_id)\n",
    "                abstract_sentences=[\"\"]   #预测用到\n",
    "                output = {\n",
    "                \"enc_len\": enc_len,\n",
    "                \"enc_input\": enc_x,\n",
    "                \"dec_input\": dec_input,\n",
    "                \"target\": dec_outputs,\n",
    "                \"dec_len\": dec_len,\n",
    "                \"article\": x,\n",
    "                \"abstract\": y,\n",
    "                \"abstract_sents\": abstract_sentences\n",
    "                }\n",
    "                yield output\n",
    "                \n",
    "            \n",
    "        elif mode==\"test\":\n",
    "            dataset_test_x=tf.data.TextLineDataset(self.test_x_path)\n",
    "            for test_d in dataset_test_x:\n",
    "                #可对每一行的enc_x进行处理\n",
    "                x=test_d.numpy().decode(\"utf-8\")\n",
    "                enc_x=x.split()[:self.max_enc_len]   #分词,限制长度处理\n",
    "                enc_len=len(enc_x)\n",
    "                enc_x=enc_x+[0]*(self.max_enc_len-enc_len)   #填充\n",
    "                enc_x=[self.vocab.word_to_id(w) for w in enc_x]  #转id\n",
    "                abstract_sentences=[]   #预测用到\n",
    "                output = {\n",
    "                \"enc_len\": enc_len,\n",
    "                \"enc_input\": enc_x,\n",
    "                \"dec_input\": [],\n",
    "                \"target\": [],\n",
    "                \"dec_len\": self.max_dec_len,\n",
    "                \"article\": x,\n",
    "                \"abstract\": '',\n",
    "                \"abstract_sents\": abstract_sentences\n",
    "                }\n",
    "                yield output\n",
    "                \n",
    "        else:\n",
    "            dataset_eval_x=tf.data.TextLineDataset(self.eval_x_path)\n",
    "            dataset_eval_y=tf.data.TextLineDataset(self.eval_y_path)\n",
    "            data_evval=tf.data.Dataset.zip((dataset_eval_x,dataset_eval_y))\n",
    "            for eval_d in data_evval:\n",
    "                #可对每一行的enc_x,dec_y进行处理\n",
    "                x,y=train_d\n",
    "                x=x.numpy().decode(\"utf-8\")  #转numpy-->str\n",
    "                y=y.numpy().decode(\"utf-8\")\n",
    "                enc_x=x.split()[:self.max_enc_len]   #分词,限制长度处理\n",
    "                enc_len=len(enc_x)\n",
    "                enc_x=enc_x+[0]*(self.max_enc_len-enc_len)   #填充\n",
    "                enc_x=[self.vocab.word_to_id(w) for w in enc_x]  #转id\n",
    "                abstract_sentences=[]   #预测用到\n",
    "                output = {\n",
    "                \"enc_len\": enc_len,\n",
    "                \"enc_input\": enc_x,\n",
    "                \"dec_input\": [],\n",
    "                \"target\": [],\n",
    "                \"dec_len\": self.max_dec_len,\n",
    "                \"article\": x,\n",
    "                \"abstract\": y,\n",
    "                \"abstract_sents\": abstract_sentences\n",
    "                }\n",
    "                yield output\n",
    "        \n",
    "        \n",
    "    def batch_generator(self,batch_size, mode):\n",
    "        dataset=tf.data.Dataset.from_generator(\n",
    "            lambda:self.example_generator(mode,batch_size),\n",
    "            output_types={\n",
    "                \"enc_len\": tf.int32,\n",
    "                \"enc_input\": tf.int32,\n",
    "                \"dec_input\": tf.int32,\n",
    "                \"target\": tf.int32,\n",
    "                \"dec_len\": tf.int32,\n",
    "                \"article\": tf.string,\n",
    "                \"abstract\": tf.string,\n",
    "                \"abstract_sents\": tf.string\n",
    "                },\n",
    "            output_shape={\n",
    "                \"enc_len\": [],\n",
    "                \"enc_input\":[None],\n",
    "                \"dec_input\": [None],\n",
    "                \"target\": [None],\n",
    "                \"dec_len\": [],\n",
    "                \"article\": [],\n",
    "                \"abstract\": [],\n",
    "                \"abstract_sents\":[None]\n",
    "                }\n",
    "        )\n",
    "        dataset=dataset.padded_batch(\n",
    "            batch_size,\n",
    "            padded_shapes={\n",
    "                \"enc_len\": [],\n",
    "                \"enc_input\":[None],\n",
    "                \"dec_input\": [self.max_dec_len],\n",
    "                \"target\": [self.max_dec_len],\n",
    "                \"dec_len\": [],\n",
    "                \"article\": [],\n",
    "                \"abstract\": [],\n",
    "                \"abstract_sents\":[None]\n",
    "            },\n",
    "            padding_values={\n",
    "                \"enc_len\": -1,\n",
    "                \"enc_input\":1,\n",
    "                \"dec_input\":1 ,\n",
    "                \"target\": 1,\n",
    "                \"dec_len\": -1,\n",
    "                \"article\": b'',\n",
    "                \"abstract\": b'',\n",
    "                \"abstract_sents\":b''\n",
    "            },\n",
    "            drop_reminder=True\n",
    "        )\n",
    "        \n",
    "        def update(entry):\n",
    "            return (\n",
    "                {\n",
    "                \"enc_len\": entry[\"enc_len\"],\n",
    "                \"enc_input\":entry[\"enc_input\"],\n",
    "                \"article\": entry[\"article\"]\n",
    "                },\n",
    "                {\n",
    "                \"dec_input\":entry[\"dec_input\"],\n",
    "                \"target\":entry[\"target\"],\n",
    "                \"dec_len\":entry[\"dec_len\"],\n",
    "                \"abstract\":entry[\"abstract\"],\n",
    "                \"abstract_sents\":entry[\"abstract_sents\"]\n",
    "                }\n",
    "            )\n",
    "        dataset=dataset.map(update)\n",
    "        return dataset\n",
    "    \n",
    "    def batcher(self,params):\n",
    "        return self.batch_generator(params[\"batch_size\"],params[\"mode\"])\n",
    "    \n",
    "    #dec文本进行处理\n",
    "    def get_dec_inp(self,seq,start_id,stop_id):\n",
    "        seq_id=[self.vocab.word_to_id(w) for w in seq]\n",
    "        dec_inp=[start_id]+seq_id\n",
    "        dec_inp=dec_inp[:self.max_dec_len]\n",
    "        dec_out=dec_inp[1:]+[stop_id]\n",
    "        assert len(dec_inp)==len(dec_out)\n",
    "        return dec_inp,dec_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、tf - tfrecord方式\n",
    "\n",
    "### 数据存入tfrecord\n",
    "   #### 1、创建writer（类似open）\n",
    "       tfrecord_writer=tf.python_io.TFRecordWriter('xxx.tfrecord')\n",
    "   #### 2、构建example（tfrecord接受的存储方式）\n",
    "       #之前还需要加数据处理等等\n",
    "       def get_tfrecord_example(x,y):\n",
    "           tfrecord_dict={}\n",
    "           #字符串\n",
    "           tfrecord_dict['sentence']=tf.train.Feature(bytes_list=tf.train.BytesList(value=[文本数据list]))\n",
    "           #int类型\n",
    "           tfrecord_dict['x']=tf.train.Feature(int64_list=tf.train.Int64List(value=[token_id list]))\n",
    "           #float类型\n",
    "           tfrecord_dict['y']=tf.train.Feature(float_list=tf.train.FloatList(value=[label_id list]))\n",
    "           #字典转feature,feature转example\n",
    "           return tf.train.Example(features=tf.train.Features(feature=tfrecord_dict))\n",
    "           \n",
    "   #### 3、example序列化\n",
    "       example=example.SerializeToString()\n",
    "   #### 4、写入并关闭\n",
    "       tfrecord_writer.write(example)\n",
    "       tfrecord_writer.close()\n",
    "    \n",
    "### 读取tfrecord+dataset封装\n",
    "  #### 1、创建dataset(类似open)\n",
    "           dataset=tf.data.TFRecordDataset(\"xxx.tfreocrd\")   #tf-dataset类型,一出手就是巅峰\n",
    "  #### 2、解析example\n",
    "          features={\n",
    "              'sentence':tf.FixedLenFeature([shape],tf.string),\n",
    "              'x':tf.FixedLenFeature([shape],tf.int64),\n",
    "              'y':tf.FixedLenFeature([shape],tf.float32)\n",
    "          }\n",
    "          def exam_parse(example,features):\n",
    "              feats=tf.parse_single_example(example,features)\n",
    "              feats['x']=tf.cast(feats['x'],tf.int32)  #\n",
    "              feats['xx']=tf.decode_raw(feats['xx'],tf.float32)    #numpy类型常用，写入时用to_bytes(str)方式\n",
    "              return feats\n",
    "  #### 3、dataset封装+操作\n",
    "          dataset.map(labmda x:exam_parse(x,features))  #转dataset\n",
    "          #map 或者 map_and_batch\n",
    "          dataset.map_and_batch(\n",
    "              labmda x:exam_parse(x,features),\n",
    "              batch_size=batch_size,\n",
    "              num_parallel_batches=num_cpu_threads,\n",
    "              drop_remainder=True\n",
    "          )\n",
    "          #dataset的其他操作：repeat()、shuffle(buffer_size)、batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存储tfrecord文件\n",
    "def get_tfrecord_example(x,y):\n",
    "       tfrecord_dict={}\n",
    "       #字符串\n",
    "       tfrecord_dict['sentence']=tf.train.Feature(bytes_list=tf.train.BytesList(value=[文本数据list]))\n",
    "       #int类型\n",
    "       tfrecord_dict['x']=tf.train.Feature(int64_list=tf.train.Int64List(value=[token_id list]))\n",
    "       #float类型\n",
    "       tfrecord_dict['y']=tf.train.Feature(float_list=tf.train.FloatList(value=[label_id list]))\n",
    "       #字典转feature,feature转example\n",
    "       return tf.train.Example(features=tf.train.Features(feature=tfrecord_dict))\n",
    "    \n",
    "tfrecord_writer=tf.python_io.TFRecordWriter('xxx.tfrecord')\n",
    "example=get_tfrecord_example(x,y)\n",
    "example=example.SerializeToString()\n",
    "tfrecord_writer.write(example)\n",
    "tfrecord_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取tfrecord文件\n",
    "dataset=tf.data.TFRecordDataset(\"xxx.tfreocrd\")   #tf-dataset类型\n",
    "features={\n",
    "          'sentence':tf.FixedLenFeature([shape],tf.string),  #根据写入的来\n",
    "          'x':tf.FixedLenFeature([shape],tf.int64),\n",
    "          'y':tf.FixedLenFeature([shape],tf.float32)\n",
    "}\n",
    "\n",
    "def exam_parse(example,features):\n",
    "  feats=tf.parse_single_example(example,features)   #解析\n",
    "  feats['x']=tf.cast(feats['x'],tf.int32)  # 转类型\n",
    "  feats['xx']=tf.decode_raw(feats['xx'],tf.float32)    #numpy类型常用，写入时用to_bytes(str)方式\n",
    "  return feats\n",
    "\n",
    "dataset=dataset.map(labmda x:exam_parse(x,features))  #转dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、pythorch 版本\n",
    "### 构建dataset类\n",
    "class xxx(Dataset):\n",
    "    \n",
    "    #目的是构建总的数据\n",
    "    def __init__(self,data_list):\n",
    "        self.data_list=data_listt\n",
    "        \n",
    "    #返回数据的长度\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    #根据index返回一条数据\n",
    "    def __getitem__(self,index):\n",
    "        pass\n",
    "   \n",
    "### 加载并清洗数据\n",
    "def load_data(file_name,max_len):\n",
    "    \n",
    "    #加载文件，抽取出文本数据\n",
    "    #截断或者pandding\n",
    "    #加入特殊字段[CLS]、[START]、[END]、[UNK]、[PAD]\n",
    "    #vocab.token_to_id操作\n",
    "    #返回特定结构\n",
    "    \n",
    "### dataloader封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e961d2fc90aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#数据封装dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xxx.xx\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xxx.xx\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMy_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#自定义dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#数据封装dataloader\n",
    "train_data=load_dataset(\"xxx.xx\",max_len=MAX_LEN)\n",
    "test_data=load_dataset(\"xxx.xx\",max_len=MAX_LEN)\n",
    "train_dataset=My_dataset(train_data)   #自定义dataset\n",
    "train_loader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "#调用:按get_item中返回格式接收\n",
    "for xx in train_loader:\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
