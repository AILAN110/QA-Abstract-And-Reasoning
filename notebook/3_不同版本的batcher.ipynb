{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练之前，进行的batch操作\n",
    "### 1、tensorflow版本的dataset\n",
    "\n",
    "### 2、pytorch版本的dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenier/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "START_DECODING = '[START]'\n",
    "STOP_DECODING = '[STOP]'\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self,vocab_path,max_size):\n",
    "        self.word2id={UNKNOWN_TOKEN:0,PAD_TOKEN:1,START_DECODING:2,STOP_DECODING:3}\n",
    "        self.id2word={0:UNKNOWN_TOKEN,1:PAD_TOKEN,2:START_DECODING,3:STOP_DECODING}\n",
    "        self.count=4\n",
    "        \n",
    "        with open(vocab_path,\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                pair=line.split()\n",
    "                if len(pair)!=2:\n",
    "                    print(\"Warning:the error line is :{}\"%line)\n",
    "                w=pair[0]\n",
    "                if w in [SENTENCE_START,SENTENCE_END,PAD_TOKEN,UNKNOWN_TOKEN,START_DECODING,STOP_DECODING]:\n",
    "                    raise Exception(\"word <%s> is exception\"%w)\n",
    "                if w in self.word2id:\n",
    "                    raise Exception(\"word <%s> is repetition\")\n",
    "                \n",
    "                if max_size>=self.count:\n",
    "                    self.word2id[w]=self.count\n",
    "                    self.id2word[self.count]=w\n",
    "                    self.count+=1\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "    def word_to_id(self,word):\n",
    "        if w not in self.word2id:\n",
    "            return self.word2id[UNKNOWN_TOKEN]\n",
    "        return self.word2id[word]\n",
    "    \n",
    "    def id_to_word(self,w_id):\n",
    "        if w_id not in self.id2word:\n",
    "            raise Exception(\"id %s can not found\"%w_id)\n",
    "        return self.id2word[w_id]\n",
    "    \n",
    "    def size(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、tf版本1\n",
    "#### 读取数据：\n",
    "    * tf.data.TextLineDataset(txt_path)----->将一句输入 整合 指定格式输出  \n",
    "    * tf.data.Dataset.zip((dataset_train_x, dataset_train_y))   #当作zip使用\n",
    "    * train_dataset.shuffle(1000, reshuffle_each_iteration=True).repeat()   #shuffle+repeat()，buffer_size=1000\n",
    "#### 将迭代器处理成dataset:\n",
    "    * tf.data.Dataset.from_generator(generator,output_types={key:tf.int32....},output_shape={key:shape...})   #generator必须是迭代器\n",
    "    为什么要再用这个？\n",
    "    答：因为上面读取数据后，还要进行train、test、eval不同状况下的处理，返回的数据不是dataset类型了，而是字典。。。。\n",
    "#### 构建batch数据,并设置默认填充：\n",
    "    * dataset.padded_batch(batch_size,padded_shapes={key:shape...},padding_values={key:value...},drop_reminder=true)\n",
    "    drop_reminder:表示最后一个不满足的batch是否丢弃\n",
    "#### map操作：\n",
    "    * 对dataset的所有输出进行格式或者映射(map)的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
